{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad539f62-2fcc-4a60-af71-6d9e5239c598",
   "metadata": {},
   "source": [
    "\n",
    "# List of Potential Topics\n",
    "# Current Trends in Technology Investment: A study about People, Companies and Investment patterns\n",
    "# Can search trends/twitter trends inform technology investment?\n",
    "# Can twitter trends inform technology investment?\n",
    "# What does the current cryptocurrency landscape look like? \n",
    "\n",
    "I hope to generate a feasible development plan of a suite of algorithms and techniques that may prove useful for technology forecasting, or investment ventures.\n",
    "\n",
    "One concept stuck with me after reading Peter Thiel's Zero to One: Notes on Startups, or How to Build the Future, and that is, making sense of the backdrop of current technology and making sense of what things are happening around you that people are talking about, that governments, investors and the market as a whole are focusing on. This project is one way I would like to express an attempt at collecting data from various sources I believe may be important to get a comprehensive understanding of the backdrop of today's current technology.\n",
    "\n",
    "Since I am interested in financial technology, I will focus on these.\n",
    "Motivation for using the following sites is driven by what is practical for myself and the sources I find useful.\n",
    "- Binance, Bloomberg\n",
    "- Regulatory filings\n",
    "- Google Cloud, Google Trends\n",
    "- Twitter\n",
    "- Wikidata to tie things up\n",
    "\n",
    "# Proposal on Data Model: Network/Graph structure\n",
    "Many real world systems can be represented as networks.[2] The motivation for this project is to test the existence of a reliable user interest network informed by two data sources: Twitter and Google Trends. \n",
    "\n",
    "\n",
    "Twitter (History limit?)\n",
    "\n",
    "Google Trends (History limit)\n",
    "\n",
    "\n",
    "In this project, the evolution of trends over time is the primary dimension of analysis, thats why we are concerned about building a temporal network. \n",
    "\n",
    "Another motivation is the extensibility of \n",
    "\n",
    "What does success look like?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "People share parts of their lives each day\n",
    "\n",
    "\n",
    "\n",
    "# Aims and Objectives\n",
    "Discover objective truths about interests\n",
    "The objective is to understand and piece up even the mundane details about a persons habits, behaviors overtime.\n",
    "\n",
    "\n",
    "- Build a temporal network\n",
    "    - Choice of storage\n",
    "    - Network configuration\n",
    "    - EDA on Network Analysis\n",
    " \n",
    "\n",
    "- Analyze technology investment patterns\n",
    "- Discover relevant technology\n",
    "\n",
    "This EDA is for the purpose of\n",
    "- Analyzing distributions from the data (mean, median, standard deviation, histograms, Cumulative distribution fnctions CDFS, Quantile-Quantile plots) to detect multimodal behavior, or a significant class of outliers. With accompanying p-value calculations\n",
    "\n",
    "- Consider outliers to prevent misconstrued analysis. \n",
    "\n",
    "- Consider noise.\n",
    "\n",
    "Describe/take into account the context/backdrop in which the data was collected. \n",
    "- Time ranges\n",
    "- Significant events (Precovid vs Covid, around movements)\n",
    "Convince TA that we did not spot any bad configurations or population restrictions (e.g Data only for chrome users)\n",
    "\n",
    "\n",
    "# Architecture Diagram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785c5118-47ee-4089-aa80-d4c634fbbf23",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Data gathering\n",
    "\n",
    "There are various data sources I am considering in order  is obtained from a reliable and data ecosystem. I intend to leverage on Google's world class .... to augment and realize the full potential of my analytic \n",
    "\n",
    "\n",
    "# Limitations of KG\n",
    "\n",
    "\n",
    "## Why Wikidata\n",
    "I intend to use it as my central source for discovery of existing external links which can enable me to further collect\n",
    "https://www.youtube.com/watch?v=Oips1aW738Q\n",
    "\n",
    "Used to reason about a particular trend as it relates to company investment activity within the same timeframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffcf9515-28a0-404d-8489-15ac4fc8a3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen('http://python.org/') as response:\n",
    "    # Read and return up to n bytes\n",
    "    html = response.read()\n",
    "#     print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a5a3c82-16cb-4db3-8d9f-108a07ea565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve resource via URL and store in temp locations\n",
    "import shutil\n",
    "import tempfile\n",
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen('http://python.org/') as response:\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        shutil.copyfileobj(response, tmp_file)\n",
    "\n",
    "with open(tmp_file.name) as html:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdeda28a-4635-44b1-a034-e3cd13b55b3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cd4e37a6ae5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtweepy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtweepy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOAuthHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from tweepy import API\n",
    "from tweepy import OAuthHandler\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def get_twitter_auth():\n",
    "    #set up twitter authentication\n",
    "\n",
    "    # Return: tweepy.OAuthHandler object\n",
    "\n",
    "    try:\n",
    "        load_dotenv()\n",
    "        consumer_key = os.environ['TWITTER_CONSUMER_KEY']\n",
    "        consumer_secret = os.environ['TWITTER_CONSUMER_SECRET']\n",
    "        access_token = os.environ['TWITTER_ACCESS_TOKEN']\n",
    "        access_secret= os.environ['TWITTER_ACCESS_SECRET']\n",
    "    except KeyError:\n",
    "        sys.stderr.write(\"TWITTER_* environment variables not set\\n\")\n",
    "        sys.exit(1)\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    return auth\n",
    "\n",
    "\n",
    "def get_twitter_client():\n",
    "    #Setu twitter API client.\n",
    "\n",
    "    # Return tweepy.API object\n",
    "\n",
    "    auth = get_twitter_auth()\n",
    "    client = API(auth)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eba669-356d-4869-bf7f-c7d073087254",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Filtering data\n",
    "Rationale behind\n",
    "\n",
    "- Specify what filtering\n",
    "\n",
    "- Counts of data being filtered at each step. Comptue all your metrics and answer questions like, What fraction of X did this filtering remove?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b851f-40f1-494b-bd63-0bd39ac9767a",
   "metadata": {},
   "source": [
    "# Data Validation\n",
    "- Validation: Is the data self-consistent. Does it repesent what I think it does? \n",
    "- Before answering the question you are interested in, rule out any other variability (did numbers of users change, did right number of affected queries show p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84c700-f148-4334-9e31-fbb1e411b524",
   "metadata": {},
   "source": [
    "# Analyzing distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aabea1-7ffc-4228-a0ba-8404033a70b1",
   "metadata": {},
   "source": [
    "# Outlier and Noise analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f477d-7e13-4b72-985b-a29725cffc9c",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "- Analysis\n",
    "- Sample your data\n",
    "- Slice your data (into preferably equal sized subgroups) along dimensions that are likely to work differently (e.g time in Days. Look at day-over-day), or even work the same and compare for internal consistency. AVOID Simpson's Paradox\n",
    "- Analyze variation/ day-over-day graphs\n",
    "- Filter your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615975b-5226-4452-91e2-ef94ec19246e",
   "metadata": {},
   "source": [
    "# Meaure twice, or more (in multiple ways)\n",
    "- Better to use differen data sources for measrements\n",
    "- Compare network with Exisiting network models \n",
    "- Check for reproducibility (slicing and consistency over time)\n",
    "- Use different time ranges or random sub samples of data to tell you how reliable/reproducible the model is (if theres a model) \n",
    "- Confirmatory analysis\n",
    "- Comparison of tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7410d39-f0a3-4502-84d3-22d5b09013c4",
   "metadata": {},
   "source": [
    "# Evaluation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49914fb5-3a57-4a4a-9339-c3a236996f07",
   "metadata": {},
   "source": [
    "# Monitoring of Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7acb6-e078-4c00-9455-20442c1c48f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37b19452-19c5-4414-850e-906d7522d887",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1 Multimodal Entity Linking on Twitter] (https://link.springer.com/chapter/10.1007/978-3-030-45439-5_31)\n",
    "\n",
    "[2] https://arxiv.org/pdf/1108.1780.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa98837-c664-4f61-99e5-2a768f4e93c6",
   "metadata": {},
   "source": [
    "7.2 7.2 Resources used\n",
    "Webscraping\n",
    "• Webscraping lecture and lab, Data Programming, Sean McGrath\n",
    "• P. de Wulf. (2021, Mar. 26). Web Scraping with BeautifulSoup [Online]. Available:\n",
    "https://www.scrapingbee.com/blog/python-web-scraping-beautiful-soup/\n",
    "• Skytowner. (2021, Mar. 18). Beautiful Soup | find_all method [Online]. Available:\n",
    "https://www.skytowner.com/explore/beautiful_soup_find_all_method\n",
    "Data cleaning and processing, feature generation\n",
    "• Natural Language Processing lab, Data Programming, Sean McGrath\n",
    "• GeeksforGeeks. (2020, Mar. 31). Find frequency of each word in a string in Python [On- line]. Available: https://www.geeksforgeeks.org/find-frequency-of-each-word-in-a-string-in-\n",
    "python/\n",
    "• K. Pykes. (2021, Mar. 17). A Guide To Cleaning Text in Python [Online]. Available:\n",
    "https://towardsdatascience.com/a-guide-to-cleaning-text-in-python-943356ac86ca\n",
    "• GeeksforGeeks. (2021, May 31). Removing stop words with NLTK in Python [Online]. Avail-\n",
    "able: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "• D. Singh. (2019, July 19). Building Features from Text Data [Online]. Available:\n",
    "https://www.pluralsight.com/guides/building-features-from-text-data\n",
    "• TutorialsPoint. Stemming & Lemmatization [Online]. Available:\n",
    "https://www.tutorialspoint.com/natural_language_toolkit/natural_language_toolkit_stemming_lemmati\n",
    "Exploratory data analysis\n",
    "• Natural Language Processing lab, Data Programming, Sean McGrath\n",
    "• K. Fessel. (2020, June 29). Introduction to Seaborn YouTube series [Online]. Available:\n",
    "https://www.youtube.com/playlist?list=PLtPIclEQf-3cG31dxSMZ8KTcDG7zYng1j\n",
    "• Matplotlib. Customizing Matplotlib with style sheets and rcParams [Online]. Available:\n",
    "https://matplotlib.org/stable/tutorials/introductory/customizing.html\n",
    "• E. Marsja. (2019, Dec. 22). How to Change the Size of Seaborn Plots [Online]. Available:\n",
    "https://www.marsja.se/how-to-change-size-of-seaborn-plot/\n",
    "• Zach. (2020, Nov. 22). How to Change Legend Font Size in Matplotlib [Online]. Available:\n",
    "https://www.statology.org/matplotlib-legend-font-size/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb8273-bdef-4db2-8944-ce9ad08835a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
